{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"NN Template","text":"<p>          \"We demand rigidly defined areas of doubt and uncertainty.\"      </p> <pre><code>cookiecutter https://github.com/grok-ai/nn-template\n</code></pre> <p></p> <p>Generic cookiecutter template to bootstrap PyTorch projects and to avoid writing boilerplate code to integrate:</p> <ul> <li>PyTorch Lightning, lightweight PyTorch wrapper for high-performance AI research.</li> <li>Hydra, a framework for elegantly configuring complex applications.</li> <li>Weights and Biases, organize and analyze machine learning experiments. (educational account available)</li> <li>Streamlit, turns data scripts into shareable web apps in minutes.</li> <li>MkDocs and Material for MkDocs, a fast, simple and downright gorgeous static site generator.</li> <li>DVC, track large files, directories, or ML models. Think \"Git for data\".</li> <li>GitHub Actions, to run the tests, publish the documentation and to PyPI automatically.</li> <li>Python best practices for developing and publishing research projects.</li> </ul> <p>cookiecutter</p> <p>This is a parametrized template that uses cookiecutter. Install cookiecutter with:</p> <p><code>pip install cookiecutter</code></p>"},{"location":"papers/","title":"Scientific Papers based on nn-template","text":"<p>The following papers acknowledge the adoption of NN Template:</p> <p>arXiv 2022</p> <p>Metric Based Few-Shot Graph Classification</p> <p>Donato Crisostomi, Simone Antonelli, Valentino Maiorca, Luca Moschella, Riccardo Marin, Emanuele Rodol\u00e0</p> <p></p> <p>Computer Graphics Forum: CGF 2022</p> <p>Learning Spectral Unions of Partial Deformable 3D Shapes</p> <p>Luca Moschella, Simone Melzi, Luca Cosmo, Filippo Maggioli, Or Litany, Maks Ovsjanikov, Leonidas Guibas, Emanuele Rodol\u00e0</p> <p></p> <p>Findings of the Association for Computational Linguistics: EMNLP 2021</p> <p>WikiNEuRal: Combined Neural and Knowledge-based Silver Data Creation for Multilingual NER</p> <p>Simone Tedeschi, Valentino Maiorca, Niccol\u00f2 Campolungo, Francesco Cecconi, and Roberto Navigli</p> <p></p> <p>Findings of the Association for Computational Linguistics: EMNLP 2021</p> <p>Named Entity Recognition for Entity Linking: What Works and What's Next.</p> <p>Simone Tedeschi, Simone Conia, Francesco Cecconi, and Roberto Navigli</p> <p></p> <p>Please let us know if your paper also does and we'll add it to the list!</p>"},{"location":"changelog/","title":"Changelog","text":"<p>See the changelog in the releases page.</p>"},{"location":"changelog/upgrade/","title":"Upgrade","text":"<p>The need for upgrading the template itself is lessened thanks to the <code>nn-template-core</code> library decoupling.</p> <p>Info</p> <p>Update the <code>nn-template-core</code> library changing the version constraint in the <code>setup.cfg</code>.</p> <p>However, you can use cruft to automate also the template updates! </p>"},{"location":"features/bestpractices/","title":"Tooling","text":"<p>The template configures are the tooling necessary for a modern Python project.</p> <p>These include:</p> <ul> <li>EditorConfig  maintain consistent coding styles for multiple developers.</li> <li>Black the uncompromising code formatter.</li> <li>isort sort imports alphabetically, and automatically separated into sections and by type.</li> <li>flake8 check coding style (PEP8), programming errors and cyclomatic complexity.</li> <li>pydocstyle static analysis tool for checking compliance with Python docstring conventions.</li> <li>MyPy static type checker for Python.</li> <li>Coverage measure code coverage of Python programs.</li> <li>bandit security linter from PyCQA.</li> <li>pre-commit framework for managing and maintaining pre-commit hooks.</li> </ul>"},{"location":"features/bestpractices/#pre-commits","title":"Pre commits","text":"<p>The pre-commits configuration is defined in <code>.pre-commit-config.yaml</code>, and includes the most important checks and auto-fix to perform.</p> <p>If one of the pre-commits fails, the commit is aborted avoiding distraction errors.</p> <p>Info</p> <p>The pre-commits are also run in the CI/CD as part of the Test Suite. This helps guaranteeing the all the contributors are respecting the code conventions in the pull requests.</p>"},{"location":"features/cicd/","title":"CI/CD","text":"<p>The generated project contains two GiHub Actions workflow to run the Test Suite and to publish you project.</p> <p>Note</p> <p>You need to enable the GitHub Actions from the settings in your repository.</p> <p>Important</p> <p>All the workflow already implement the logic needed to cache the conda and pip environment between workflow runs.</p> <p>Warning</p> <p>The annotated tags in the git repository to manage releases should follow the semantic versioning conventions: <code>&lt;major&gt;.&lt;minor&gt;.&lt;patch&gt;</code></p>"},{"location":"features/cicd/#test-suite","title":"Test Suite","text":"<p>The Test Suite runs automatically for each commit in a Pull Request. It is successful if:</p> <ul> <li>The pre-commits do not raise any errors</li> <li>All the tests pass</li> </ul> <p>After that, the PR are marked with \u2714\ufe0f or \u274c depending on the test suite results.</p>"},{"location":"features/cicd/#publish-docs","title":"Publish docs","text":"<p>The first time you should use <code>mike</code> to: create the <code>gh-pages</code> branch and specify the default docs version.</p> <pre><code>mike deploy 0.1 latest --push\nmike set-default latest\n</code></pre> <p>Warning</p> <p>You do not need to execute these commands if you accepted the optional cookiecutter setup step.</p> <p>Info</p> <p>Remember to enable the GitHub Pages from the repository settings.</p> <p>After that, the docs are built and automatically published <code>on release</code> on GitHub Pages. This means that every time you publish a new release in your project an associated version of the documentation is published.</p> <p>Important</p> <p>The documentation version utilizes only the <code>&lt;major&gt;.&lt;minor&gt;</code> version of the release tag, discarding the patch version.</p>"},{"location":"features/cicd/#publish-pypi","title":"Publish PyPi","text":"<p>To publish your package on PyPi it is enough to configure the PyPi token in the GitHub repository <code>secrets</code> and de-comment the following in the <code>publish.yaml</code> workflow:</p> <pre><code>      - name: Build SDist and wheel\nrun: pipx run build\n- name: Check metadata\nrun: pipx run twine check dist/*\n- name: Publish distribution \ud83d\udce6 to PyPI\nuses: pypa/gh-action-pypi-publish@release/v1\nwith:\nuser: __token__\npassword: ${{ secrets.PYPI_API_TOKEN }}\n</code></pre> <p>In this way, on each GitHub release the package gets published on PyPi and the associated documentation is published on GitHub Pages.</p>"},{"location":"features/conda/","title":"Python Environment","text":"<p>The generated project is a Python Package, whose dependencies are defined in the <code>setup.cfg</code> The development setup comprises a <code>conda</code> environment which installs the package itself in edit mode.</p>"},{"location":"features/conda/#dependencies","title":"Dependencies","text":"<p>All the project dependencies should be defined in the <code>setup.cfg</code> as <code>pip</code> dependencies. In rare cases, it is useful to specify conda dependencies --- they will not be resolved when installing the package from PyPi.</p> <p>This division is useful when installing particular or optimized packages such a <code>PyTorch</code> and PyTorch Geometric.</p> <p>Hint</p> <p>It is possible to manage the Python version to use in the conda <code>env.yaml</code>.</p> <p>Info</p> <p>This organization allows for <code>conda</code> and <code>pip</code> dependencies to co-exhist, which in practice happens a lot in research projects.</p>"},{"location":"features/conda/#update","title":"Update","text":"<p>In order to update the <code>pip</code> dependencies after changing the <code>setup.cfg</code> it is enough to run:</p> <pre><code>pip install -e '.[dev]'\n</code></pre>"},{"location":"features/determinism/","title":"Determinism","text":"<p>The template always logs the seed utilized in order to guarantee reproducibility.</p> <p>The user specifies a <code>seed_index</code> value in the configuration <code>train/default.yaml</code>:</p> <pre><code>seed_index: 1\ndeterministic: False\n</code></pre> <p>This value indexes an array of deterministic but randomly generated seeds, e.g.:</p> <pre><code>Setting seed 1273642419 from seeds[1]\n</code></pre> <p>Hint</p> <p>This setup allows to easily run the same experiment with different seeds in a reproducible way. It is enough to run a Hydra multi-run over the <code>seed_index</code>.</p> <p>The following would run the same experiment with five different seeds, which can be analyzed in the logger dashboard:</p> <pre><code>python src/project/run.py -m train.seed_index=1,2,3,4\n</code></pre> <p>Info</p> <p>The deterministic option <code>deterministic: False</code> controls the use of deterministic algorithms in PyTorch, it is forwarded to the Lightning Trainer.</p>"},{"location":"features/docs/","title":"Documentation","text":"<p><code>MkDocs</code> and <code>Material for MkDocs</code> is already configured in the generated project.</p> <p>In order to create your docs it is enough to:</p> <ol> <li> <p>Modify the <code>nav</code> index in the <code>mkdocs.yaml</code>, which describes how to organize the pages.    An example of the <code>nav</code> is the following:</p> <pre><code>nav:\n- Home: index.md\n- Getting started:\n- Generating your project: getting-started/generation.md\n- Strucure: getting-started/structure.md\n</code></pre> </li> <li> <p>Create all the files referenced in the <code>nav</code> relative to the <code>docs/</code> folder.</p> <pre><code>\u276f tree docs\ndocs\n\u251c\u2500\u2500 getting-started\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 generation.md\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 structure.md\n\u2514\u2500\u2500 index.md\n</code></pre> </li> <li> <p>To preview your documentation it is enough to run <code>mkdocs serve</code>. To manually deploy the documentation     see <code>mike</code>, or see the integrated GitHub Action to publish the docs on release.</p> </li> </ol>"},{"location":"features/envvars/","title":"Environment Variables","text":"<p>System specific variables (e.g. absolute paths) should not be under version control, otherwise there will be conflicts between different users.</p> <p>The best way to handle system specific variables is through environment variables.</p> <p>You can define new environment variables in a <code>.env</code> file in the project root. A copy of this file (e.g. <code>.env.template</code>) can be under version control to ease new project configurations.</p> <p>To define a new variable write inside <code>.env</code>:</p> <pre><code>export MY_VAR=/home/user/my_system_path\n</code></pre> <p>You can dynamically resolve the variable name from everywhere</p> pythonyamlposix <p>In Python code use:</p> <pre><code>get_env(\"MY_VAR\")\n</code></pre> <p>In the Hydra <code>yaml</code> configurations:</p> <pre><code>${oc.env:MY_VAR}\n</code></pre> <p>In posix shells:</p> <pre><code>. .env\necho $MY_VAR\n</code></pre>"},{"location":"features/fastdevrun/","title":"Fast Dev Run","text":"<p>The template expands the Lightning <code>fast_dev_run</code> mode to be more debugging friendly.</p> <p>It will also:</p> <ul> <li>Disable multiple workers in the dataloaders</li> <li>Use the CPU and not the GPU</li> </ul> <p>Info</p> <p>It is possible to modify this behaviour by simply modifying the <code>run.py</code> file.</p>"},{"location":"features/metadata/","title":"MetaData","text":"<p>The bridge between the Lightning DataModule and the Lightning Module.</p> <p>It is responsible for collecting data information to be fed to the module. The Lightning Module will receive an instance of MetaData when instantiated, both in the train loop or when restored from a checkpoint.</p> <p>Warning</p> <p>MetaData exposes <code>save</code> and <code>load</code>. Those are two user-defined methods that specify how to serialize and de-serialize the information contained in its attributes. This is needed for the checkpointing restore to work properly and must be always implemented, where the metadata is needed.</p> <p>This decoupling allows the architecture to be parametric (e.g. in the number of classes) and DataModule/Trainer independent (useful in prediction scenarios). Examples are the class names in a classification task or the vocabulary in NLP tasks.</p>"},{"location":"features/nncore/","title":"NN Template core","text":"<p>Most of the logic is abstracted from the template into an accompanying library: <code>nn-template-core</code>.</p> <p>This library contains the logic necessary for the restore, logging, and many other functionalities implemented in the template.</p> <p>Info</p> <p>This decoupling eases the updating of the template, reaching a desirable tradeoff:</p> <ul> <li><code>template</code>: easy to use and customize, hard to update</li> <li><code>library</code>: hard to customize, easy to update</li> </ul> <p>With our approach updating most of the functions is extremely easy, it is just a Python dependency, while maintaing the flexibility of a template.</p> <p>Warning</p> <p>It is important to not remove the <code>NNTemplateCore</code> callback from the instantiated callbacks in the template. It is used to inject personalized behaviour in the training loop.</p>"},{"location":"features/restore/","title":"Restore","text":"<p>The template offers a way to restore a previous run from the configuration. The relevant configuration block is in <code>conf/train/default.yml</code>:</p> <pre><code>restore:\nckpt_or_run_path: null\nmode: null # null, finetune, hotstart, continue\n</code></pre>"},{"location":"features/restore/#ckpt_or_run_path","title":"ckpt_or_run_path","text":"<p>The <code>ckpt_or_run_path</code> can be a path towards a Lightning Checkpoint or the run identifiers w.r.t. the logger. In case of W&amp;B as a logger, they are called <code>run_path</code> and are in the form of <code>entity/project/run_id</code>.</p> <p>Warning</p> <p>If <code>ckpt_or_run_path</code> points to a checkpoint, that checkpoint must have been saved with this template, because additional information are attached to the checkpoint to guarantee a correct restore. These include the <code>run_path</code> itself and the whole configuration used.</p>"},{"location":"features/restore/#mode","title":"mode","text":"<p>We support 4 different modes for restoring an experiment:</p> nullfinetunehotstartcontinue <p><pre><code>restore:\nmode: null\n</code></pre> In this <code>mode</code> no restore happens, and <code>ckpt_or_run_path</code> is ignored.</p> <p>Use Case</p> <p>This is the default option and allows the user to train the model from scratch logging into a new run.</p> <p><pre><code>restore:\nmode: finetune\n</code></pre> In this <code>mode</code> only the model weights are restored, both the <code>Trainer</code> state and the logger run are not restored.</p> <p>Use Case</p> <p>As the name suggest, one of the most common use case is when fine tuning a trained model logging into a new run with a novel training regimen.</p> <p><pre><code>restore:\nmode: hotstart\n</code></pre> In this <code>mode</code> the training continues from the checkpoint restoring the <code>Trainer</code> state but the logging does not. A new run is created on the logger dashboard.</p> <p>Use Case</p> <p>Perform different tests in separate logging runs branching from the same trained model.</p> <p><pre><code>restore:\nmode: continue\n</code></pre> In this <code>mode</code> the training continues from the checkpoint and the logging continues in the previous run. No new run is created on the logger dashboard.</p> <p>Use Case</p> <p>The training execution was interrupted and the user wants to continue it.</p> <p>Restore summary</p> null finetune hotstart continue Model weights Trainer state Logging run"},{"location":"features/storage/","title":"Storage","text":"<p>The checkpoints and other data produces by the experiment is stored in a logger agnostic folder defined in the configuration <code>core.storage_dir</code></p> <p>This is the organization of the <code>storage_dir</code>:</p> <pre><code>storage\n\u2514\u2500\u2500 &lt;project_name&gt;\n    \u2514\u2500\u2500  &lt;run_id&gt;\n         \u251c\u2500\u2500 checkpoints\n         \u2502    \u2514\u2500\u2500 &lt;checkpoint_name&gt;.ckpt.zip\n         \u2514\u2500\u2500 config.yaml\n</code></pre> <p>In the configuration it is possible to specify whether the run files stored inside the <code>storage_dir</code> should be uploaded to the cloud:</p> <pre><code>logging:\nupload:\nrun_files: true\nsource: true\n</code></pre>"},{"location":"features/tags/","title":"Tags","text":"<p>Each run should be <code>tagged</code> in order to easily filter them from the logged dashboard. Unfortunately, it is easy to forget to tag correctly each run.</p> <p>We ask interactively for a list of comma separated tags, if those are not already defined in the configuration: <pre><code>WARNING  No tags provided, asking for tags...\nEnter a list of comma separated tags (develop):\n</code></pre></p> <p>Info</p> <p>If the current experiment is a sweep comprised of multiple runs and there are not any tags defined, an error is raised instead: <pre><code>ERROR    You need to specify 'core.tags' in a multi-run setting!\n</code></pre></p>"},{"location":"features/tests/","title":"Tests","text":"<p>The generated project includes automated tests that use the current configuration defined in your project.</p> <p>You should write additional tests specific to each project, but running the tests should give an idea at least if the code and fundamental operations work as expected.</p> <p>Info</p> <p>You can execute the tests with:</p> <pre><code>pytest -v\n</code></pre>"},{"location":"getting-started/","title":"Principles behind nn-template","text":"<p>When developing neural models ourselves, we often struggled with:</p> <ul> <li>Reproducibility. We strongly believe in the reproducibility requirement of scientific work.</li> <li>Framework Learning. Even when you find (or code yourself) the best framework to fit your needs, you still end up   in messy situations when collaborating since others have to learn to use it;</li> <li>Avoiding boilerplate. We were bored to write the same code over and over in     every project to handle the typical ML pipeline.</li> </ul> <p>Over the course of the years, we fine-tuned our toolbox to reach this local minimum with respect to our self-imposed requirements. After many epochs of training, the result is nn-template.</p> <p>nn-template is not a framework</p> <ul> <li>It does not aim to sidestep the need to write code.</li> <li>It does not constrain your workflow more than PyTorch Lightning does.</li> </ul>"},{"location":"getting-started/generation/","title":"Initial Setup","text":""},{"location":"getting-started/generation/#cookiecutter","title":"Cookiecutter","text":"<p><code>nn-template</code> is, by definition, a template to generate projects. It's a robust starting point for your projects, something that lets you skip the initial boilerplate in configuring the environment, tests and such. Since it is a blueprint to build upon, it has no utility in being installed via pip or similar tools.</p> <p>Instead, we rely on cookiecutter to manage the setup stages and deliver to you a ready-to-run project. It is a general-purpose tool that enables users to add their water of choice (variable configurations) to their particular Cup-a-Soup (the template to be setup).</p> <p>Installing cookiecutter</p> <p><code>cookiecutter</code> can be installed via pip in any Python-enabled environment (it won't be the same used by the project once instantiated). Our advice is to install <code>cookiecutter</code> as a system utility via pipx:</p> <pre><code>pipx install cookiecutter\n</code></pre> <p>Then, we need to tell cookiecutter which template to work on:</p> <pre><code>cookiecutter https://github.com/grok-ai/nn-template.git\n</code></pre> <p>It will clone the nn-template repository in the background, call its interactive setup, and build your project's folder according to the given parametrization.</p> <p>The parametrized setup will take care of:</p> <ul> <li>Set up the development of a Python package</li> <li>Initializing a clean Git repository and add the GitHub remote of choice</li> <li>Create a new Conda environment to execute your code in</li> </ul> <p>This extra step via cookiecutter is done to avoid a lot of manual parametrization, unavoidable when cloning a template repository from scratch. Trust us, it is totally worth the bother!</p>"},{"location":"getting-started/generation/#building-blocks","title":"Building Blocks","text":"<p>The generated project already contains a minimal working example. You are free to modify anything you want except for a few essential and high-level things that keep everything working. (again, this is not a framework!). In particular mantain:</p> <ul> <li>Any <code>LightningLogger</code> you may want to use wrapped in a <code>NNLogger</code></li> <li>The <code>NNTemplateCore</code> Lightning callback</li> </ul> <p>Hint</p> <p>The template bootstraps the project with most of the needed boilerplate. The remaining components to implement for your project are the following:</p> <ol> <li>Implement data pipeline<ol> <li>Dataset</li> <li>Pytorch Lightning DataModule</li> </ol> </li> <li>Implement neural modules<ol> <li>Model</li> <li>Pytorch Lightning Module</li> </ol> </li> </ol>"},{"location":"getting-started/generation/#faqs","title":"FAQs","text":"What is The Answer to the Ultimate Question of Life, the Universe, and Everything? <p>42</p> Why are the logs badly formatted in PyCharm? <p>This is due to the fact that we are using Rich to handle the logging, and Rich is not compatible with customized terminals. As its documentation says:</p> <p>\"PyCharm users will need to enable \u201cemulate terminal\u201d in output console option in run/debug configuration to see styled output.\"</p> Why are file paths not interactive in the terminal's output? <p>We would like to know, too.</p> How can I exclude specific file paths from pre-commit checks (e.g. pydocstyle)? <p>While we encourage everyone to keep best-practices and standards enforced via the pre-commit utility, we also take into account situations where you just copy/paste code from the Internet and fixing it would be tedious. In those cases, the file <code>.pre-commit-config.yaml</code> has you covered. Each hook can receive an additional property, namely <code>exclude</code> where you can specify single files or patterns to be excluded when running that hook.</p> <p>For example, if you want to exclude a file named <code>ugly_but_working_code.py</code> from an annoying hook <code>annoying_hook</code> (most likely <code>pydocstyle</code>): <pre><code>  - repo: https://github.com/slow_coding/annoying_hook.git\nhooks:\n-   id: annoying_hook\nexclude: ugly_but_working_code.py\n</code></pre></p>"},{"location":"getting-started/generation/#future-features","title":"Future Features","text":"<ul> <li> Optuna support</li> <li> Support different loggers other than WandB</li> </ul>"},{"location":"integrations/dvc/","title":"Data Version Control","text":"<p>DVC runs alongside <code>git</code> and uses the current commit hash to version control the data.</p> <p>Initialize the <code>dvc</code> repository:</p> <pre><code>$ dvc init\n</code></pre> <p>To start tracking a file or directory, use <code>dvc add</code>:</p> <pre><code>$ dvc add data/ImageNet\n</code></pre> <p>DVC stores information about the added file (or a directory) in a special <code>.dvc</code> file named <code>data/ImageNet.dvc</code>, a small text file with a human-readable format. This file can be easily versioned like source code with Git, as a placeholder for the original data (which gets listed in <code>.gitignore</code>):</p> <pre><code>git add data/ImageNet.dvc data/.gitignore\ngit commit -m \"Add raw data\"\n</code></pre>"},{"location":"integrations/dvc/#making-changes","title":"Making changes","text":"<p>When you make a change to a file or directory, run <code>dvc add</code> again to track the latest version:</p> <pre><code>$ dvc add data/ImageNet\n</code></pre>"},{"location":"integrations/dvc/#switching-between-versions","title":"Switching between versions","text":"<p>The regular workflow is to use <code>git checkout</code> first to switch a branch, checkout a commit, or a revision of a <code>.dvc</code> file, and then run <code>dvc checkout</code> to sync data:</p> <pre><code>$ git checkout &lt;...&gt;\n$ dvc checkout\n</code></pre> <p>Info</p> <p>Read more in the DVC docs!</p>"},{"location":"integrations/githubactions/","title":"GitHub Actions","text":"<p>Automate, customize, and execute your software development workflows right in your repository with GitHub Actions.</p> <p>Info</p> <p>The template offers workflows to automatically run tests and pre-commits on pull requests, publish on PyPi and the docs on GitHub Pages on release.</p>"},{"location":"integrations/hydra/","title":"Hydra","text":"<p>Hydra is an open-source Python framework that simplifies the development of research and other complex applications. The key feature is the ability to dynamically create a hierarchical configuration by composition and override it through config files and the command line. The name Hydra comes from its ability to run multiple similar jobs - much like a Hydra with multiple heads.</p> <p>The basic functionalities are intuitive: it is enough to change the configuration files in <code>conf/*</code> accordingly to your preferences. Everything will be logged in <code>wandb</code> automatically.</p> <p>Consider creating new root configurations <code>conf/myawesomeexp.yaml</code> instead of always using the default <code>conf/default.yaml</code>.</p>"},{"location":"integrations/hydra/#multi-run","title":"Multi-run","text":"<p>You can easily perform hyperparameters sweeps, which override the configuration defined in <code>/conf/*</code>.</p> <p>The easiest one is the grid-search. It executes the code with every possible combinations of the specified hyperparameters:</p> <pre><code>python src/run.py -m optim.optimizer.lr=0.02,0.002,0.0002 optim.lr_scheduler.T_mult=1,2 optim.optimizer.weight_decay=0,1e-5\n</code></pre> <p>You can explore aggregate statistics or compare and analyze each run in the W&amp;B dashboard.</p> <p>Info</p> <p>We recommend to go through at least the Basic Tutorial, keep in mind that Hydra builds on top of OmegaConf.</p>"},{"location":"integrations/lightning/","title":"PyTorch Lightning","text":"<p>Lightning makes coding complex networks simple. It is not a high level framework like <code>keras</code>, but forces a neat code organization and encapsulation.</p> <p>You should be somewhat familiar with PyTorch and PyTorch Lightning before using this template.</p> <p></p>"},{"location":"integrations/mkdocs/","title":"MkDocs","text":"<p>MkDocs is a fast, simple and downright gorgeous static site generator that's geared towards building project documentation.</p> <p>Documentation source files are written in Markdown, and configured with a single YAML configuration file.</p>"},{"location":"integrations/mkdocs/#material-for-mkdocs","title":"Material for MkDocs","text":"<p>Material for MkDocs is a theme for MkDocs, a static site generator geared towards (technical) project documentation.</p> <p>Hint</p> <p>The template comes with Material for MkDocs already configured, to create your documentation you only need to write markdown files and define the <code>nav</code>.</p> <p>See the Documentation page to get started!</p>"},{"location":"integrations/streamlit/","title":"Streamlit","text":"<p>Streamlit is an open-source Python library that makes it easy to create and share beautiful, custom web apps for machine learning and data science.</p> <p>In just a few minutes, you can build and deploy powerful data apps to:</p> <ul> <li>Explore your data</li> <li>Interact with your model</li> <li>Analyze your model behavior and input sensitivity</li> <li>Showcase your prototype with awesome web apps</li> </ul> <p>Moreover, Streamlit enables interactive development with automatic rerun on files changes.</p> <p></p> <p>Info</p> <p>Launch a minimal app with <code>PYTHONPATH=. streamlit run src/ui/run.py</code>. There is a built-in function to restore a model checkpoint stored on W&amp;B, with automatic download if the checkpoint is not present in the local machine:</p>"},{"location":"integrations/wandb/","title":"Weights and Biases","text":"<p>Weights &amp; Biases helps you keep track of your machine learning projects. Use tools to log hyperparameters and output metrics from your runs, then visualize and compare results and quickly share findings with your colleagues.</p> <p>This is an example of a simple dashboard.</p>"},{"location":"integrations/wandb/#quickstart","title":"Quickstart","text":"<p>Login to your <code>wandb</code> account, running once <code>wandb login</code>. Configure the logging in <code>conf/logging/*</code>.</p> <p>Info</p> <p>Read more in the docs. Particularly useful the <code>log</code> method, accessible from inside a PyTorch Lightning module with <code>self.logger.experiment.log</code>.</p>"},{"location":"project-structure/structure/","title":"Structure","text":"<pre><code>.\n\u251c\u2500\u2500 conf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 default.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hydra\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 default.yaml\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 nn\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 default.yaml\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 train\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 default.yaml\n\u251c\u2500\u2500 data\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 .gitignore\n\u251c\u2500\u2500 docs\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 index.md\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 overrides\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 main.html\n\u251c\u2500\u2500 .editorconfig\n\u251c\u2500\u2500 .env\n\u251c\u2500\u2500 .env.template\n\u251c\u2500\u2500 env.yaml\n\u251c\u2500\u2500 .flake8\n\u251c\u2500\u2500 .github\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 workflows\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 publish.yml\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 test_suite.yml\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 mkdocs.yml\n\u251c\u2500\u2500 .pre-commit-config.yaml\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 setup.cfg\n\u251c\u2500\u2500 setup.py\n\u251c\u2500\u2500 src\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 awesome_project\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 data\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 datamodule.py\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 dataset.py\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 modules\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 module.py\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 pl_modules\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 pl_module.py\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 run.py\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 ui\n\u2502\u00a0\u00a0         \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 run.py\n\u2514\u2500\u2500 tests\n    \u251c\u2500\u2500 conftest.py\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 test_checkpoint.py\n    \u251c\u2500\u2500 test_configuration.py\n    \u251c\u2500\u2500 test_nn_core_integration.py\n    \u251c\u2500\u2500 test_resume.py\n    \u251c\u2500\u2500 test_storage.py\n    \u2514\u2500\u2500 test_training.py\n</code></pre>"}]}