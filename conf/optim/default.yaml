# @package _group_

optimizer:
  _target_: torch.optim.Adam
  lr: 0.002
  betas: [0.9, 0.999]
  eps: 1e-08
  weight_decay: 0

use_lr_scheduler: True
lr_scheduler:
  _target_: torch.optim.lr_scheduler.CosineAnnealingWarmRestarts
  T_0: 10
  T_mult: 1
  eta_min: 1e-4
  last_epoch: -1
  verbose: True
